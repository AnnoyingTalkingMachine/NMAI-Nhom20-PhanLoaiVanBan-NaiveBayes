{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chuẩn hóa dữ liệu tiếng việt -> đưa về dạng từ điển</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Đồng bộ unicode</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    " \n",
    "dicchar = loaddicchar()\n",
    " \n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenize<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Remove HTML Code</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bk hn test'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)\n",
    "\n",
    "txt = \"<p class=\\\"par\\\">bk hn test</p>\"\n",
    "remove_html(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Clean Token</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(token):\n",
    "    if(token == \"k\" or token == \"ko\" or token == \"khong\" or token == \"kg\"): return \"không\"\n",
    "    if(token == \"dc\" or token == \"đc\"): return \"được\"\n",
    "    if(token == \"sp\"): return \"sản_phẩm\"\n",
    "    if(token == \"e\"): return \"em\"\n",
    "    if(token == 'a'): return \"anh\"\n",
    "    if(token == \"sz\"): return \"size\"\n",
    "    if(token == \"siu\"): return \"siêu\"\n",
    "    if(token == \"cx\"): return \"cũng\"\n",
    "    if(token == \"bt\"): return \"bình_thường\"\n",
    "    if(token == \"iu\"): return \"yêu\"\n",
    "    if(token == 'i'): return \"y\"\n",
    "    if(token == \"oke\" or token == \"okie\" or token ==\"okz\"): return \"ok\"\n",
    "    if(token == \"xau\"): return \"xấu\"\n",
    "    if(token == \"tks\"): return \"thanks\"\n",
    "    if(token == \"dep\"): return \"đẹp\"\n",
    "    if(token == \"tam\"): return \"tạm\"\n",
    "    if(token == \"loi\"): return \"lỗi\"\n",
    "    if(token == \"hai long\"): return \"hài_lòng\"\n",
    "    if(token == \"tot\"): return \"tốt\"\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Remove stopwords<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words =  pd.read_csv('./input/vietnamese-stopwords.txt', header = None)\n",
    "\n",
    "stop_word = set(words.values.ravel())\n",
    "final_stop_word = set()\n",
    "\n",
    "for i in stop_word:\n",
    "    a = i.replace(\" \",\"_\")\n",
    "    final_stop_word.add(a)\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in final_stop_word:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Xóa các kí tự thừa</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_character(document):\n",
    "    \n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]',' ',document)\n",
    "    document = re.sub(\"[0-9].kg\", \"\", document) #remove cân nặng\n",
    "    document = re.sub(\"[0-9].k\", \"\", document) #remove giá tiền\n",
    "    document = re.sub(\"[0-9]\", \"\", document) #remove number\n",
    "    document = re.sub(\"nn*\", \"n\", document) #rm nn\n",
    "    document = re.sub(\"gg*\", \"g\", document) #rm gg\n",
    "    document = re.sub(\"uu*\", \"u\", document) #rm uu\n",
    "    document = re.sub(\"pp*\", \"p\", document) #rm pp\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Đưa về dạng từ điển</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w ã á'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1= 'w ã á'\n",
    "remove_redundant_character(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict_tokens(cleaned_tokens):\n",
    "        newDict = dict()\n",
    "\n",
    "        for token in cleaned_tokens:\n",
    "            if token in newDict:\n",
    "                newDict[token] += 1\n",
    "            else:\n",
    "                newDict[token] = 1\n",
    "\n",
    "        return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = remove_redundant_character(document)\n",
    "\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "\n",
    "    # tokenize\n",
    "    document = ViTokenizer.tokenize(document)\n",
    "    document = word_tokenize(document)\n",
    "    \n",
    "    # clean tokens\n",
    "    for i in range(0, len(document)):\n",
    "        document[i] = clean_token(document[i])\n",
    "    document = \" \".join(str(e) for e in document)\n",
    "\n",
    "    #remove stopwords\n",
    "    document = remove_stopwords(document)\n",
    "\n",
    "    # đưa về dạng từ điển\n",
    "    document = document.split(' ')\n",
    "    document = list_to_dict_tokens(document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Process Train Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('./input/dataVNese.csv', encoding = \"UTF-8\")\n",
    "\n",
    "# lấy số lượng các câu ở mỗi label bằng nhau (4700 câu/1 label) #\n",
    "df_raw.drop(df_raw.iloc[:, 3:], axis = 1, inplace = True)\n",
    "\n",
    "df_raw_neg = df_raw[df_raw['label'] == 'NEG' ]\n",
    "df_raw_neg = df_raw_neg.iloc[:4700]\n",
    "\n",
    "df_raw_neu = df_raw[df_raw['label'] == 'NEU']\n",
    "\n",
    "df_raw_4 = df_raw[df_raw['rate'] == 4]\n",
    "df_raw_5 = df_raw[df_raw['rate'] == 5]\n",
    "\n",
    "df_raw_4 = df_raw_4.iloc[:2350]\n",
    "df_raw_5 = df_raw_5.iloc[:2350]\n",
    "\n",
    "df_raw_pos = pd.concat([df_raw_4, df_raw_5])\n",
    "dataraw_final = pd.concat([df_raw_neg, df_raw_neu, df_raw_pos])\n",
    "\n",
    "dataraw_final = dataraw_final.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xử lý dữ liệu train #\n",
    "process_comment = []\n",
    "comment = dataraw_final.comment\n",
    "for item in comment:\n",
    "    a = text_preprocess(item)\n",
    "    process_comment.append(a)\n",
    "\n",
    "list_tuples = list(zip(process_comment, dataraw_final.label)) \n",
    "processed_data = pd.DataFrame(list_tuples, columns = ['comment', 'label'])\n",
    "\n",
    "x_train = processed_data['comment']\n",
    "y_train = processed_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lưu file x_train và y_train#\n",
    "import pickle\n",
    "\n",
    "with open('x_train.pkl', 'wb') as f:\n",
    "     pickle.dump(x_train, f)\n",
    "with open('y_train.pkl', 'wb') as f:\n",
    "     pickle.dump(y_train, f)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Process Test Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_raw = pd.read_csv('./input/test_concat.csv', encoding='utf-8')\n",
    "data_test_raw.drop(data_test_raw.columns[2], axis= 1, inplace=True)\n",
    "\n",
    "# Gán nhãn dữ liệu #\n",
    "label = []\n",
    "for item in data_test_raw['True_Label']:\n",
    "    if(item == 5 or item == 4): \n",
    "        a = \"POS\"\n",
    "        label.append(a)\n",
    "    elif (item == 3):\n",
    "        a = \"NEU\"\n",
    "        label.append(a)\n",
    "    else:\n",
    "        a = \"NEG\"\n",
    "        label.append(a)\n",
    "\n",
    "\n",
    "data_test_raw['label'] = label\n",
    "\n",
    "data_test_raw.drop(data_test_raw.columns[1], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy số lượng câu ở mỗi nhãn là như nhau#\n",
    "test_raw_neg = data_test_raw[data_test_raw['label'] == 'NEG' ]\n",
    "\n",
    "test_raw_neu = data_test_raw[data_test_raw['label'] == 'NEU']\n",
    "\n",
    "test_raw_pos = data_test_raw[data_test_raw['label'] == 'POS']\n",
    "\n",
    "test_raw_pos = test_raw_pos.iloc[:705]\n",
    "test_raw_neg = test_raw_neg.iloc[:705]\n",
    "\n",
    "test_raw_final = pd.concat([test_raw_pos, test_raw_neu, test_raw_neg])\n",
    "\n",
    "test_raw_final = test_raw_final.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xử lý dữ liệu test#\n",
    "test_comment = []\n",
    "comment = test_raw_final.comment\n",
    "for item in comment:\n",
    "    a = text_preprocess(item)\n",
    "    test_comment.append(a)\n",
    "\n",
    "list_tuples2 = list(zip(test_comment, test_raw_final.label)) \n",
    "processed_data = pd.DataFrame(list_tuples2, columns = ['comment', 'label'])\n",
    "\n",
    "x_test = processed_data['comment']\n",
    "y_test = processed_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lưu file x_test và y_test  #\n",
    "import pickle\n",
    "with open('x_test.pkl', 'wb') as f:\n",
    "     pickle.dump(x_test, f)\n",
    "with open('y_test.pkl', 'wb') as f:\n",
    "     pickle.dump(y_test, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc file train#\n",
    "with open('x_train.pkl', 'rb') as f:\n",
    "      x_train1 = pickle.load(f)\n",
    "with open('y_train.pkl', 'rb') as f:\n",
    "      y_train1 = pickle.load(f)\n",
    "\n",
    "# Đọc file test #\n",
    "with open('x_test.pkl', 'rb') as f:\n",
    "      x_test1 = pickle.load(f)\n",
    "with open('y_test.pkl', 'rb') as f:\n",
    "      y_test1 = pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd8ac7ed5329ff1f59a8356dfddf06d159cf18414acbfd7b13d1d5cd1cbb5d7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
