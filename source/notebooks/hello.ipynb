{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RAW\n",
    "dataFrame_raw = pd.read_csv(\"..\\\\..\\\\data\\\\raw\\\\dataset.csv\", encoding=\"ISO-8859-1\", header=None)\n",
    "dataFrame_raw.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "\n",
    "dataFrame = dataFrame_raw[[\"label\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cắt nhỏ kích thước dữ liệu\n",
    "dataFrame_positive = dataFrame[dataFrame[\"label\"] == 4]\n",
    "dataFrame_negative = dataFrame[dataFrame[\"label\"] == 0]\n",
    "\n",
    "dataFrame_positive = dataFrame_positive.iloc[:int(len(dataFrame_positive) / 40)]\n",
    "dataFrame_negative = dataFrame_negative.iloc[:int(len(dataFrame_negative) / 40)]\n",
    "\n",
    "dataFrame = pd.concat([dataFrame_positive, dataFrame_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tk = TweetTokenizer(reduce_len=True)\n",
    "data = []\n",
    "\n",
    "for index, df in dataFrame.iterrows():\n",
    "    if df[\"label\"] == 4:\n",
    "        data.append( (tk.tokenize(df[\"text\"]), 1) )\n",
    "    else:\n",
    "        data.append( (tk.tokenize(df[\"text\"]), 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_token(token): \n",
    "    token = [token] \n",
    "    token = pos_tag(token)\n",
    "\n",
    "    if token[0][1].startswith(\"NN\"):\n",
    "        pos = 'n'\n",
    "    elif token[0][1].startswith(\"VB\"):\n",
    "        pos = 'v'\n",
    "    else:\n",
    "        pos = 'a'\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(token[0][0], pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "def expand_token(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '½25':\n",
    "        return ''\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'three', 'year']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove noise\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def clean_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token in tweet_tokens:\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\",'', token)\n",
    "        \n",
    "        if len(token) >= 1:\n",
    "            token = expand_token(token.lower())\n",
    "\n",
    "            token = lemmatize_token(token)\n",
    "        \n",
    "            if token not in string.punctuation and token not in STOP_WORDS:\n",
    "                cleaned_tokens.append(token)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "clean_tokens([\"he\", \"wouldn't\", \"love\", \"u\", \"in\", \"three\", \"yrs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'love': True, 'guy': True, 'best': True}, 1), ({'im': True, 'meeting': True, 'one': True, 'besties': True, 'tonight': True, 'cant': True, 'wait': True, 'girl': True, 'talk': True}, 1), ({'thanks': True, 'twitter': True, 'add': True, 'sunisa': True, 'get': True, 'meet': True, 'hin': True, 'show': True, 'dc': True, 'area': True, 'sweetheart': True}, 1)]\n"
     ]
    }
   ],
   "source": [
    "def list_to_dict(cleaned_tokens):\n",
    "    return dict([token, True] for token in cleaned_tokens)\n",
    "\n",
    "final_data = []\n",
    "\n",
    "for tokens, label in data:\n",
    "    final_data.append((list_to_dict(clean_tokens(tokens)), label))\n",
    "\n",
    "print(final_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72d35e0c788eed675a538630783dd674bb786417aa5acd52bed34fb439755cd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
